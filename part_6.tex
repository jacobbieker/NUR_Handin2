\section{Part 6 Classifying Gamma-ray Bursts}

The file of the functions used for this exercise is:

\lstinputlisting{part_six.py}

The result of this function is shown in

\lstinputlisting{6.txt}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.9\linewidth]{./plots/GRB_Error.png}
  \caption{Mean Square Error during training on GRB.}
  \label{fig:mse}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.9\linewidth]{./plots/GRB_Misclassification.png}
  \caption{GRB Misclassification Rate over training.}
  \label{fig:misclassify}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.9\linewidth]{./plots/GRB_histogram.png}
  \caption{GRB Histogram Real Class vs Predicted}
  \label{fig:predction}
\end{figure}

As can be seen in \ref{fig:predction}, the logistic regression does not do a great
job of separating the two classes, and there does not seem to be enough
information for the network to properly learn a hyperplane that can
separate the two classes.

Additionally, the network requires a long time to train, so the
weights are loaded from the disk from a previous training run. Missing data is not
changed at all, with the idea that the network will learn to ignore the -1. values as not
being correlated with the short or long classification.

